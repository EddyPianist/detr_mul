[11/07 11:46:50.715]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[11/07 11:46:50.716]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50_2 --batch_size 2 --epochs 12 --lr_drop 10 --coco_path /home/mmcom/COCO --use_dn
[11/07 11:46:50.716]: world size: 2
[11/07 11:46:50.716]: rank: 1
[11/07 11:46:50.716]: local_rank: 1
[11/07 11:46:50.716]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=12, eval=False, find_unused_params=False, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=10, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50_2', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[11/07 11:47:19.908]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[11/07 11:47:19.908]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50_2 --batch_size 2 --epochs 12 --lr_drop 10 --coco_path /home/mmcom/COCO --use_dn
[11/07 11:47:19.909]: world size: 2
[11/07 11:47:19.909]: rank: 1
[11/07 11:47:19.909]: local_rank: 1
[11/07 11:47:19.909]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=12, eval=False, find_unused_params=False, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=10, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50_2', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[11/07 11:49:56.816]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[11/07 11:49:56.817]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50_2 --batch_size 2 --epochs 12 --lr_drop 10 --coco_path /home/mmcom/COCO --use_dn
[11/07 11:49:56.817]: world size: 2
[11/07 11:49:56.817]: rank: 1
[11/07 11:49:56.817]: local_rank: 1
[11/07 11:49:56.817]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=12, eval=False, find_unused_params=False, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=10, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50_2', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[11/07 11:49:57.873]: number of params:46541185
[11/07 11:49:57.875]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[11/07 11:52:46.750]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[11/07 11:52:46.750]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50_2 --batch_size 2 --epochs 12 --lr_drop 10 --coco_path /home/mmcom/COCO --use_dn
[11/07 11:52:46.751]: world size: 2
[11/07 11:52:46.751]: rank: 1
[11/07 11:52:46.751]: local_rank: 1
[11/07 11:52:46.751]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=12, eval=False, find_unused_params=False, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=10, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50_2', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[11/07 11:52:47.869]: number of params:46541185
[11/07 11:52:47.871]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[11/07 11:55:20.425]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[11/07 11:55:20.425]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50_2 --batch_size 2 --epochs 12 --lr_drop 10 --coco_path /home/mmcom/COCO --use_dn
[11/07 11:55:20.425]: world size: 2
[11/07 11:55:20.425]: rank: 1
[11/07 11:55:20.425]: local_rank: 1
[11/07 11:55:20.426]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=12, eval=False, find_unused_params=False, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=10, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50_2', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[11/07 11:55:21.478]: number of params:46541185
[11/07 11:55:21.480]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[11/07 11:59:26.480]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[11/07 11:59:26.480]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50_2 --batch_size 2 --epochs 12 --lr_drop 10 --coco_path /home/mmcom/COCO --use_dn
[11/07 11:59:26.480]: world size: 2
[11/07 11:59:26.480]: rank: 1
[11/07 11:59:26.480]: local_rank: 1
[11/07 11:59:26.480]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=12, eval=False, find_unused_params=False, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=10, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50_2', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[11/07 11:59:27.532]: number of params:46541185
[11/07 11:59:27.534]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[11/07 12:00:52.450]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[11/07 12:00:52.451]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50_2 --batch_size 2 --epochs 12 --lr_drop 10 --coco_path /home/mmcom/COCO --use_dn
[11/07 12:00:52.451]: world size: 2
[11/07 12:00:52.451]: rank: 1
[11/07 12:00:52.451]: local_rank: 1
[11/07 12:00:52.451]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=12, eval=False, find_unused_params=False, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=10, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50_2', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[11/07 12:00:53.506]: number of params:46541185
[11/07 12:00:53.509]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[11/07 12:03:53.209]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[11/07 12:03:53.210]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50_2 --batch_size 2 --epochs 12 --lr_drop 10 --coco_path /home/mmcom/COCO --use_dn
[11/07 12:03:53.210]: world size: 2
[11/07 12:03:53.210]: rank: 1
[11/07 12:03:53.210]: local_rank: 1
[11/07 12:03:53.210]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=12, eval=False, find_unused_params=False, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=10, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50_2', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[11/07 12:03:54.264]: number of params:46541185
[11/07 12:03:54.267]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[11/07 12:06:14.174]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[11/07 12:06:14.174]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50_2 --batch_size 2 --epochs 12 --lr_drop 10 --coco_path /home/mmcom/COCO --use_dn
[11/07 12:06:14.174]: world size: 2
[11/07 12:06:14.174]: rank: 1
[11/07 12:06:14.174]: local_rank: 1
[11/07 12:06:14.175]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=12, eval=False, find_unused_params=False, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=10, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50_2', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[11/07 12:06:15.228]: number of params:46541185
[11/07 12:06:15.230]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[11/07 12:11:50.034]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[11/07 12:11:50.035]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50_2 --batch_size 2 --epochs 12 --lr_drop 10 --coco_path /home/mmcom/COCO --use_dn
[11/07 12:11:50.035]: world size: 2
[11/07 12:11:50.035]: rank: 1
[11/07 12:11:50.035]: local_rank: 1
[11/07 12:11:50.035]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=12, eval=False, find_unused_params=False, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=10, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50_2', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[11/07 12:11:51.138]: number of params:46541185
[11/07 12:11:51.140]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[11/07 12:15:37.554]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[11/07 12:15:37.554]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50_2 --batch_size 2 --epochs 12 --lr_drop 10 --coco_path /home/mmcom/COCO --use_dn
[11/07 12:15:37.555]: world size: 2
[11/07 12:15:37.555]: rank: 1
[11/07 12:15:37.555]: local_rank: 1
[11/07 12:15:37.555]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=12, eval=False, find_unused_params=False, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=10, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50_2', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[11/07 12:15:38.606]: number of params:46541185
[11/07 12:15:38.608]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[11/07 12:17:48.630]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[11/07 12:17:48.631]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50_2 --batch_size 2 --epochs 12 --lr_drop 10 --coco_path /home/mmcom/COCO --use_dn
[11/07 12:17:48.631]: world size: 2
[11/07 12:17:48.631]: rank: 1
[11/07 12:17:48.631]: local_rank: 1
[11/07 12:17:48.631]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=12, eval=False, find_unused_params=False, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=10, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50_2', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[11/07 12:17:49.701]: number of params:46541185
[11/07 12:17:49.704]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[11/08 10:41:50.019]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[11/08 10:41:50.019]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50_2 --batch_size 2 --epochs 12 --lr_drop 10 --coco_path /home/mmcom/COCO --use_dn
[11/08 10:41:50.019]: world size: 2
[11/08 10:41:50.020]: rank: 1
[11/08 10:41:50.020]: local_rank: 1
[11/08 10:41:50.020]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=12, eval=False, find_unused_params=False, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=10, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50_2', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[11/08 10:41:51.092]: number of params:46541185
[11/08 10:41:51.095]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[11/08 11:15:06.683]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[11/08 11:15:06.683]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50_2 --batch_size 2 --epochs 12 --lr_drop 10 --coco_path /home/mmcom/COCO --use_dn
[11/08 11:15:06.683]: world size: 2
[11/08 11:15:06.683]: rank: 1
[11/08 11:15:06.683]: local_rank: 1
[11/08 11:15:06.684]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=12, eval=False, find_unused_params=False, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=10, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50_2', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[11/08 11:15:07.736]: number of params:46541185
[11/08 11:15:07.738]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[11/08 11:24:16.337]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[11/08 11:24:16.337]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50_2 --batch_size 2 --epochs 12 --lr_drop 10 --coco_path /home/mmcom/COCO --use_dn
[11/08 11:24:16.337]: world size: 2
[11/08 11:24:16.337]: rank: 1
[11/08 11:24:16.337]: local_rank: 1
[11/08 11:24:16.337]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=12, eval=False, find_unused_params=False, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=10, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50_2', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[11/08 11:24:17.390]: number of params:46541185
[11/08 11:24:17.393]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[11/08 11:27:32.177]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[11/08 11:27:32.178]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50_2 --batch_size 2 --epochs 12 --lr_drop 10 --coco_path /home/mmcom/COCO --use_dn
[11/08 11:27:32.178]: world size: 2
[11/08 11:27:32.178]: rank: 1
[11/08 11:27:32.178]: local_rank: 1
[11/08 11:27:32.178]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=12, eval=False, find_unused_params=False, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=10, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50_2', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[11/08 11:27:33.233]: number of params:46541185
[11/08 11:27:33.235]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[11/08 12:15:27.505]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[11/08 12:15:27.505]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50_2 --batch_size 2 --epochs 12 --lr_drop 10 --coco_path /home/mmcom/COCO --use_dn
[11/08 12:15:27.505]: world size: 2
[11/08 12:15:27.505]: rank: 1
[11/08 12:15:27.505]: local_rank: 1
[11/08 12:15:27.506]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=12, eval=False, find_unused_params=False, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=10, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50_2', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[11/08 12:15:28.567]: number of params:46541185
[11/08 12:15:28.570]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[11/09 10:23:47.802]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[11/09 10:23:47.802]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50_2 --batch_size 2 --epochs 12 --lr_drop 10 --coco_path /home/mmcom/COCO --use_dn
[11/09 10:23:47.802]: world size: 2
[11/09 10:23:47.802]: rank: 1
[11/09 10:23:47.802]: local_rank: 1
[11/09 10:23:47.803]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=12, eval=False, find_unused_params=False, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=10, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50_2', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[11/09 10:23:48.855]: number of params:46541185
[11/09 10:23:48.857]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[11/09 10:32:14.912]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[11/09 10:32:14.912]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50_2 --batch_size 2 --epochs 12 --lr_drop 10 --coco_path /home/mmcom/COCO --use_dn
[11/09 10:32:14.912]: world size: 2
[11/09 10:32:14.912]: rank: 1
[11/09 10:32:14.912]: local_rank: 1
[11/09 10:32:14.912]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=12, eval=False, find_unused_params=False, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=10, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50_2', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[11/09 10:32:15.966]: number of params:46541185
[11/09 10:32:15.969]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[11/09 10:33:14.061]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[11/09 10:33:14.061]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50_2 --batch_size 2 --epochs 12 --lr_drop 10 --coco_path /home/mmcom/COCO --use_dn
[11/09 10:33:14.061]: world size: 2
[11/09 10:33:14.061]: rank: 1
[11/09 10:33:14.061]: local_rank: 1
[11/09 10:33:14.062]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=12, eval=False, find_unused_params=False, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=10, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50_2', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[11/09 10:33:15.118]: number of params:46541185
[11/09 10:33:15.121]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[11/09 10:36:10.478]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[11/09 10:36:10.478]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50_2 --batch_size 2 --epochs 12 --lr_drop 10 --coco_path /home/mmcom/COCO --use_dn
[11/09 10:36:10.478]: world size: 2
[11/09 10:36:10.478]: rank: 1
[11/09 10:36:10.478]: local_rank: 1
[11/09 10:36:10.478]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=12, eval=False, find_unused_params=False, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=10, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50_2', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[11/09 10:36:11.540]: number of params:46541185
[11/09 10:36:11.543]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[11/09 10:37:58.169]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[11/09 10:37:58.169]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50_2 --batch_size 2 --epochs 12 --lr_drop 10 --coco_path /home/mmcom/COCO --use_dn
[11/09 10:37:58.170]: world size: 2
[11/09 10:37:58.170]: rank: 1
[11/09 10:37:58.170]: local_rank: 1
[11/09 10:37:58.170]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=12, eval=False, find_unused_params=False, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=10, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50_2', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[11/09 10:37:59.222]: number of params:46541185
[11/09 10:37:59.225]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[11/09 10:54:50.589]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[11/09 10:54:50.589]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50_2 --batch_size 2 --epochs 12 --lr_drop 10 --coco_path /home/mmcom/COCO --use_dn
[11/09 10:54:50.589]: world size: 2
[11/09 10:54:50.589]: rank: 1
[11/09 10:54:50.589]: local_rank: 1
[11/09 10:54:50.590]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=12, eval=False, find_unused_params=False, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=10, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50_2', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[11/09 10:54:51.643]: number of params:46541185
[11/09 10:54:51.645]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[11/09 11:00:14.256]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[11/09 11:00:14.257]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50_2 --batch_size 2 --epochs 12 --lr_drop 10 --coco_path /home/mmcom/COCO --use_dn
[11/09 11:00:14.257]: world size: 2
[11/09 11:00:14.257]: rank: 1
[11/09 11:00:14.257]: local_rank: 1
[11/09 11:00:14.257]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=12, eval=False, find_unused_params=False, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=10, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50_2', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[11/09 11:00:15.318]: number of params:46541185
[11/09 11:00:15.320]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[11/09 11:06:56.528]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[11/09 11:06:56.528]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50_2 --batch_size 2 --epochs 12 --lr_drop 10 --coco_path /home/mmcom/COCO --use_dn
[11/09 11:06:56.528]: world size: 2
[11/09 11:06:56.528]: rank: 1
[11/09 11:06:56.528]: local_rank: 1
[11/09 11:06:56.529]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=12, eval=False, find_unused_params=False, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=10, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50_2', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[11/09 11:06:57.580]: number of params:46541185
[11/09 11:06:57.583]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[11/09 11:08:50.036]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[11/09 11:08:50.036]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50_2 --batch_size 2 --epochs 12 --lr_drop 10 --coco_path /home/mmcom/COCO --use_dn
[11/09 11:08:50.036]: world size: 2
[11/09 11:08:50.037]: rank: 1
[11/09 11:08:50.037]: local_rank: 1
[11/09 11:08:50.037]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=12, eval=False, find_unused_params=False, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=10, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50_2', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[11/09 11:08:51.089]: number of params:46541185
[11/09 11:08:51.091]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[11/09 11:18:42.454]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[11/09 11:18:42.454]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50_2 --batch_size 2 --epochs 12 --lr_drop 10 --coco_path /home/mmcom/COCO --use_dn
[11/09 11:18:42.454]: world size: 2
[11/09 11:18:42.454]: rank: 1
[11/09 11:18:42.454]: local_rank: 1
[11/09 11:18:42.454]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=12, eval=False, find_unused_params=False, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=10, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50_2', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[11/09 11:18:43.526]: number of params:46935937
[11/09 11:18:43.528]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.0.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.0.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.0.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.0.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.0.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.0.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.0.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.0.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.1.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.1.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.1.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.1.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.1.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.1.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.1.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.1.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.2.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.2.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.2.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.2.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.2.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.2.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.2.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.2.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.3.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.3.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.3.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.3.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.3.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.3.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.3.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.3.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.4.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.4.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.4.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.4.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.4.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.4.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.4.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.4.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.5.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.5.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.5.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.5.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.5.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.5.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.5.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.5.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[11/09 11:38:05.752]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[11/09 11:38:05.752]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50_2 --batch_size 2 --epochs 12 --lr_drop 10 --coco_path /home/mmcom/COCO --use_dn
[11/09 11:38:05.752]: world size: 2
[11/09 11:38:05.752]: rank: 1
[11/09 11:38:05.752]: local_rank: 1
[11/09 11:38:05.753]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=12, eval=False, find_unused_params=False, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=10, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50_2', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[11/09 11:38:06.814]: number of params:46935937
[11/09 11:38:06.817]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.0.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.0.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.0.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.0.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.0.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.0.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.0.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.0.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.1.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.1.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.1.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.1.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.1.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.1.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.1.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.1.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.2.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.2.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.2.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.2.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.2.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.2.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.2.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.2.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.3.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.3.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.3.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.3.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.3.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.3.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.3.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.3.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.4.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.4.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.4.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.4.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.4.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.4.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.4.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.4.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.5.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.5.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.5.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.5.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.5.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.5.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.5.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.5.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[11/09 11:45:51.360]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[11/09 11:45:51.361]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50_2 --batch_size 2 --epochs 12 --lr_drop 10 --coco_path /home/mmcom/COCO --use_dn
[11/09 11:45:51.361]: world size: 2
[11/09 11:45:51.361]: rank: 1
[11/09 11:45:51.361]: local_rank: 1
[11/09 11:45:51.361]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=12, eval=False, find_unused_params=False, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=10, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50_2', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[11/09 11:45:52.431]: number of params:46935937
[11/09 11:45:52.434]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.0.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.0.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.0.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.0.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.0.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.0.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.0.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.0.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.1.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.1.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.1.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.1.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.1.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.1.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.1.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.1.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.2.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.2.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.2.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.2.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.2.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.2.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.2.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.2.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.3.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.3.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.3.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.3.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.3.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.3.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.3.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.3.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.4.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.4.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.4.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.4.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.4.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.4.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.4.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.4.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.5.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.5.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.5.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.5.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.5.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.5.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.5.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.5.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[11/09 11:47:11.797]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[11/09 11:47:11.798]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50_2 --batch_size 2 --epochs 12 --lr_drop 10 --coco_path /home/mmcom/COCO --use_dn
[11/09 11:47:11.798]: world size: 2
[11/09 11:47:11.798]: rank: 1
[11/09 11:47:11.798]: local_rank: 1
[11/09 11:47:11.798]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=12, eval=False, find_unused_params=False, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=10, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50_2', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[11/09 11:47:12.895]: number of params:46935937
[11/09 11:47:12.898]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.0.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.0.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.0.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.0.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.0.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.0.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.0.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.0.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.1.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.1.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.1.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.1.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.1.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.1.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.1.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.1.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.2.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.2.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.2.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.2.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.2.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.2.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.2.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.2.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.3.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.3.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.3.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.3.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.3.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.3.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.3.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.3.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.4.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.4.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.4.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.4.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.4.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.4.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.4.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.4.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.5.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.5.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.5.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.5.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.5.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.5.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.5.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.5.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
