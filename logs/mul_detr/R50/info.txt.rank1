[11/10 09:57:40.327]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[11/10 09:57:40.328]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50 --batch_size 2 --epochs 12 --lr_drop 10 --coco_path /home/mmcom/COCO --use_dn
[11/10 09:57:40.328]: world size: 2
[11/10 09:57:40.328]: rank: 1
[11/10 09:57:40.328]: local_rank: 1
[11/10 09:57:40.329]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=12, eval=False, find_unused_params=False, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=10, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[11/10 09:57:44.006]: number of params:46935937
[11/10 09:57:44.012]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.0.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.0.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.0.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.0.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.0.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.0.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.0.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.0.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.1.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.1.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.1.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.1.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.1.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.1.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.1.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.1.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.2.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.2.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.2.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.2.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.2.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.2.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.2.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.2.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.3.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.3.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.3.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.3.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.3.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.3.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.3.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.3.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.4.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.4.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.4.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.4.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.4.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.4.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.4.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.4.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.5.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.5.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.5.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.5.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.5.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.5.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.5.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.5.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[11/10 10:54:04.261]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[11/10 10:54:04.262]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50 --batch_size 2 --epochs 12 --lr_drop 10 --coco_path /home/mmcom/COCO --use_dn
[11/10 10:54:04.262]: world size: 2
[11/10 10:54:04.262]: rank: 1
[11/10 10:54:04.262]: local_rank: 1
[11/10 10:54:04.263]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=12, eval=False, find_unused_params=False, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=10, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[11/10 10:55:34.481]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[11/10 10:55:34.482]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50 --batch_size 2 --epochs 12 --lr_drop 10 --coco_path /home/mmcom/COCO --use_dn
[11/10 10:55:34.482]: world size: 2
[11/10 10:55:34.482]: rank: 1
[11/10 10:55:34.482]: local_rank: 1
[11/10 10:55:34.483]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=12, eval=False, find_unused_params=False, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=10, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[11/10 10:56:54.148]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[11/10 10:56:54.149]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50 --batch_size 2 --epochs 12 --lr_drop 10 --coco_path /home/mmcom/COCO --use_dn
[11/10 10:56:54.149]: world size: 2
[11/10 10:56:54.149]: rank: 1
[11/10 10:56:54.149]: local_rank: 1
[11/10 10:56:54.150]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=12, eval=False, find_unused_params=False, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=10, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[11/10 10:56:56.738]: number of params:48116097
[11/10 10:56:56.744]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.0.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.0.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.0.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.0.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.0.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.0.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.0.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.0.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.1.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.1.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.1.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.1.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.1.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.1.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.1.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.1.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.2.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.2.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.2.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.2.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.2.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.2.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.2.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.2.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.3.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.3.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.3.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.3.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.3.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.3.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.3.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.3.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.4.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.4.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.4.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.4.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.4.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.4.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.4.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.4.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.5.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.5.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.5.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.5.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.5.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.5.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.5.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.5.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.transformer.pan.downsample.0.weight": 589824,
  "module.transformer.pan.downsample.0.bias": 256,
  "module.transformer.pan.downsample.1.weight": 589824,
  "module.transformer.pan.downsample.1.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[11/10 11:00:25.694]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[11/10 11:00:25.694]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50 --batch_size 2 --epochs 12 --lr_drop 10 --coco_path /home/mmcom/COCO --use_dn
[11/10 11:00:25.695]: world size: 2
[11/10 11:00:25.695]: rank: 1
[11/10 11:00:25.695]: local_rank: 1
[11/10 11:00:25.696]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=12, eval=False, find_unused_params=False, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=10, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[11/10 11:00:28.264]: number of params:48116097
[11/10 11:00:28.270]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.0.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.0.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.0.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.0.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.0.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.0.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.0.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.0.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.1.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.1.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.1.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.1.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.1.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.1.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.1.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.1.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.2.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.2.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.2.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.2.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.2.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.2.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.2.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.2.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.3.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.3.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.3.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.3.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.3.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.3.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.3.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.3.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.4.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.4.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.4.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.4.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.4.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.4.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.4.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.4.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.5.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.5.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.5.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.5.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.5.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.5.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.5.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.5.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.transformer.pan._downsample.0.weight": 589824,
  "module.transformer.pan._downsample.0.bias": 256,
  "module.transformer.pan._downsample.1.weight": 589824,
  "module.transformer.pan._downsample.1.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[11/10 11:04:25.057]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[11/10 11:04:25.058]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50 --batch_size 2 --epochs 12 --lr_drop 10 --coco_path /home/mmcom/COCO --use_dn
[11/10 11:04:25.058]: world size: 2
[11/10 11:04:25.058]: rank: 1
[11/10 11:04:25.058]: local_rank: 1
[11/10 11:04:25.059]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=12, eval=False, find_unused_params=False, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=10, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[11/10 11:04:27.761]: number of params:48116097
[11/10 11:04:27.768]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.0.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.0.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.0.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.0.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.0.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.0.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.0.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.0.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.1.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.1.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.1.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.1.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.1.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.1.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.1.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.1.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.2.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.2.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.2.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.2.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.2.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.2.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.2.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.2.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.3.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.3.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.3.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.3.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.3.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.3.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.3.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.3.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.4.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.4.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.4.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.4.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.4.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.4.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.4.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.4.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.5.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.5.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.5.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.5.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.5.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.5.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.5.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.5.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.transformer.pan._downsample.0.weight": 589824,
  "module.transformer.pan._downsample.0.bias": 256,
  "module.transformer.pan._downsample.1.weight": 589824,
  "module.transformer.pan._downsample.1.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[11/10 11:13:00.424]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[11/10 11:13:00.425]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50 --batch_size 2 --epochs 12 --lr_drop 10 --coco_path /home/mmcom/COCO --use_dn
[11/10 11:13:00.425]: world size: 2
[11/10 11:13:00.425]: rank: 1
[11/10 11:13:00.425]: local_rank: 1
[11/10 11:13:00.426]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=12, eval=False, find_unused_params=False, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=10, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[11/10 11:13:03.237]: number of params:48116097
[11/10 11:13:03.243]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.0.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.0.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.0.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.0.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.0.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.0.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.0.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.0.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.1.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.1.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.1.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.1.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.1.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.1.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.1.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.1.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.2.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.2.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.2.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.2.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.2.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.2.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.2.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.2.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.3.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.3.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.3.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.3.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.3.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.3.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.3.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.3.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.4.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.4.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.4.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.4.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.4.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.4.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.4.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.4.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.5.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.5.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.5.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.5.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.5.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.5.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.5.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.5.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.transformer.pan._downsample.0.weight": 589824,
  "module.transformer.pan._downsample.0.bias": 256,
  "module.transformer.pan._downsample.1.weight": 589824,
  "module.transformer.pan._downsample.1.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[11/10 11:17:26.064]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[11/10 11:17:26.065]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50 --batch_size 2 --epochs 12 --lr_drop 10 --coco_path /home/mmcom/COCO --use_dn
[11/10 11:17:26.065]: world size: 2
[11/10 11:17:26.066]: rank: 1
[11/10 11:17:26.066]: local_rank: 1
[11/10 11:17:26.067]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=12, eval=False, find_unused_params=False, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=10, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[11/10 11:17:28.926]: number of params:48116097
[11/10 11:17:28.933]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.0.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.0.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.0.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.0.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.0.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.0.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.0.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.0.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.1.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.1.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.1.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.1.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.1.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.1.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.1.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.1.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.2.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.2.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.2.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.2.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.2.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.2.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.2.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.2.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.3.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.3.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.3.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.3.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.3.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.3.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.3.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.3.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.4.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.4.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.4.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.4.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.4.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.4.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.4.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.4.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.5.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.5.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.5.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.5.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.5.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.5.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.5.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.5.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.transformer.pan._downsample.0.weight": 589824,
  "module.transformer.pan._downsample.0.bias": 256,
  "module.transformer.pan._downsample.1.weight": 589824,
  "module.transformer.pan._downsample.1.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[11/10 11:22:40.689]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[11/10 11:22:40.690]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50 --batch_size 2 --epochs 12 --lr_drop 10 --coco_path /home/mmcom/COCO --use_dn
[11/10 11:22:40.690]: world size: 2
[11/10 11:22:40.690]: rank: 1
[11/10 11:22:40.690]: local_rank: 1
[11/10 11:22:40.691]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=12, eval=False, find_unused_params=False, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=10, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[11/10 11:22:43.384]: number of params:48116097
[11/10 11:22:43.390]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.0.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.0.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.0.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.0.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.0.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.0.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.0.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.0.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.1.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.1.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.1.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.1.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.1.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.1.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.1.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.1.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.2.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.2.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.2.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.2.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.2.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.2.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.2.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.2.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.3.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.3.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.3.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.3.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.3.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.3.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.3.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.3.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.4.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.4.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.4.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.4.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.4.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.4.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.4.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.4.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.5.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.5.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.5.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.5.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.5.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.5.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.5.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.5.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.transformer.pan._downsample.0.weight": 589824,
  "module.transformer.pan._downsample.0.bias": 256,
  "module.transformer.pan._downsample.1.weight": 589824,
  "module.transformer.pan._downsample.1.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[11/10 11:34:31.128]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[11/10 11:34:31.129]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50 --batch_size 2 --epochs 12 --lr_drop 10 --coco_path /home/mmcom/COCO --use_dn --find_unused_params
[11/10 11:34:31.129]: world size: 2
[11/10 11:34:31.129]: rank: 1
[11/10 11:34:31.130]: local_rank: 1
[11/10 11:34:31.131]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=12, eval=False, find_unused_params=True, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=10, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[11/10 11:34:33.846]: number of params:48116097
[11/10 11:34:33.854]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.0.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.0.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.0.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.0.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.0.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.0.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.0.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.0.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.1.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.1.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.1.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.1.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.1.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.1.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.1.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.1.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.2.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.2.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.2.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.2.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.2.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.2.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.2.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.2.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.3.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.3.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.3.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.3.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.3.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.3.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.3.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.3.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.4.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.4.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.4.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.4.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.4.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.4.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.4.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.4.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.5.def_cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.5.def_cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.5.def_cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.5.def_cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.5.def_cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.5.def_cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.5.def_cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.5.def_cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.transformer.pan._downsample.0.weight": 589824,
  "module.transformer.pan._downsample.0.bias": 256,
  "module.transformer.pan._downsample.1.weight": 589824,
  "module.transformer.pan._downsample.1.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[11/10 12:09:50.472]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[11/10 12:09:50.473]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50 --batch_size 2 --epochs 12 --lr_drop 10 --coco_path /home/mmcom/COCO --use_dn --find_unused_params
[11/10 12:09:50.473]: world size: 2
[11/10 12:09:50.474]: rank: 1
[11/10 12:09:50.474]: local_rank: 1
[11/10 12:09:50.474]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=12, eval=False, find_unused_params=True, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=10, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[11/10 12:09:53.031]: number of params:45681793
[11/10 12:09:53.037]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.transformer.pan._downsample.0.weight": 589824,
  "module.transformer.pan._downsample.0.bias": 256,
  "module.transformer.pan._downsample.1.weight": 589824,
  "module.transformer.pan._downsample.1.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[11/10 12:14:07.469]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[11/10 12:14:07.470]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50 --batch_size 2 --epochs 12 --lr_drop 10 --coco_path /home/mmcom/COCO --use_dn --find_unused_params
[11/10 12:14:07.470]: world size: 2
[11/10 12:14:07.470]: rank: 1
[11/10 12:14:07.470]: local_rank: 1
[11/10 12:14:07.471]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=12, eval=False, find_unused_params=True, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=10, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[11/10 12:14:09.942]: number of params:45681793
[11/10 12:14:09.948]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.transformer.pan._downsample.0.weight": 589824,
  "module.transformer.pan._downsample.0.bias": 256,
  "module.transformer.pan._downsample.1.weight": 589824,
  "module.transformer.pan._downsample.1.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[11/10 12:19:57.895]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[11/10 12:19:57.896]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50 --batch_size 2 --epochs 12 --lr_drop 10 --coco_path /home/mmcom/COCO --use_dn --find_unused_params
[11/10 12:19:57.896]: world size: 2
[11/10 12:19:57.896]: rank: 1
[11/10 12:19:57.897]: local_rank: 1
[11/10 12:19:57.897]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=12, eval=False, find_unused_params=True, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=10, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[11/10 12:20:00.356]: number of params:45681793
[11/10 12:20:00.362]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.transformer.pan._downsample.0.weight": 589824,
  "module.transformer.pan._downsample.0.bias": 256,
  "module.transformer.pan._downsample.1.weight": 589824,
  "module.transformer.pan._downsample.1.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[11/10 12:24:05.471]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[11/10 12:24:05.472]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50 --batch_size 2 --epochs 12 --lr_drop 10 --coco_path /home/mmcom/COCO --use_dn --find_unused_params
[11/10 12:24:05.472]: world size: 2
[11/10 12:24:05.472]: rank: 1
[11/10 12:24:05.472]: local_rank: 1
[11/10 12:24:05.473]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=12, eval=False, find_unused_params=True, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=10, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[11/10 12:24:07.927]: number of params:45681793
[11/10 12:24:07.933]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 49152,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 192,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.weight": 24576,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.bias": 96,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.transformer.pan._downsample.0.weight": 589824,
  "module.transformer.pan._downsample.0.bias": 256,
  "module.transformer.pan._downsample.1.weight": 589824,
  "module.transformer.pan._downsample.1.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[12/09 13:55:41.238]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[12/09 13:55:41.270]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50 --batch_size 2 --epochs 50 --lr_drop 40 --coco_path /home/mmcom/COCO --use_dn --find_unused_params
[12/09 13:55:41.270]: world size: 2
[12/09 13:55:41.270]: rank: 1
[12/09 13:55:41.270]: local_rank: 1
[12/09 13:55:41.271]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=50, eval=False, find_unused_params=True, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=40, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[12/09 13:55:44.573]: number of params:46882497
[12/09 13:55:44.579]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.transformer.pan._downsample.0.weight": 589824,
  "module.transformer.pan._downsample.0.bias": 256,
  "module.transformer.pan._downsample.1.weight": 589824,
  "module.transformer.pan._downsample.1.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[12/09 14:02:00.196]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[12/09 14:02:00.197]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50 --batch_size 2 --epochs 50 --lr_drop 40 --coco_path /home/mmcom/COCO --use_dn --find_unused_params
[12/09 14:02:00.197]: world size: 2
[12/09 14:02:00.197]: rank: 1
[12/09 14:02:00.198]: local_rank: 1
[12/09 14:02:00.198]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=50, eval=False, find_unused_params=True, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=40, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[12/09 14:02:03.017]: number of params:46882497
[12/09 14:02:03.023]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.transformer.pan._downsample.0.weight": 589824,
  "module.transformer.pan._downsample.0.bias": 256,
  "module.transformer.pan._downsample.1.weight": 589824,
  "module.transformer.pan._downsample.1.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[12/09 14:10:38.819]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[12/09 14:10:38.820]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50 --batch_size 2 --epochs 50 --lr_drop 40 --coco_path /home/mmcom/COCO --use_dn --find_unused_params
[12/09 14:10:38.820]: world size: 2
[12/09 14:10:38.820]: rank: 1
[12/09 14:10:38.821]: local_rank: 1
[12/09 14:10:38.821]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=50, eval=False, find_unused_params=True, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=40, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[12/09 14:10:41.355]: number of params:46882497
[12/09 14:10:41.361]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.transformer.pan._downsample.0.weight": 589824,
  "module.transformer.pan._downsample.0.bias": 256,
  "module.transformer.pan._downsample.1.weight": 589824,
  "module.transformer.pan._downsample.1.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[12/09 14:12:32.258]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[12/09 14:12:32.259]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50 --batch_size 2 --epochs 50 --lr_drop 40 --coco_path /home/mmcom/COCO --use_dn --find_unused_params
[12/09 14:12:32.260]: world size: 2
[12/09 14:12:32.260]: rank: 1
[12/09 14:12:32.260]: local_rank: 1
[12/09 14:12:32.261]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=50, eval=False, find_unused_params=True, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=40, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[12/09 14:12:34.751]: number of params:46882497
[12/09 14:12:34.757]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.transformer.pan._downsample.0.weight": 589824,
  "module.transformer.pan._downsample.0.bias": 256,
  "module.transformer.pan._downsample.1.weight": 589824,
  "module.transformer.pan._downsample.1.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[12/09 14:15:50.860]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[12/09 14:15:50.861]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50 --batch_size 2 --epochs 50 --lr_drop 40 --coco_path /home/mmcom/COCO --use_dn --find_unused_params
[12/09 14:15:50.861]: world size: 2
[12/09 14:15:50.862]: rank: 1
[12/09 14:15:50.862]: local_rank: 1
[12/09 14:15:50.862]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=50, eval=False, find_unused_params=True, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.0001, lr_backbone=1e-05, lr_drop=40, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[12/09 14:15:53.358]: number of params:46882497
[12/09 14:15:53.364]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.0.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.1.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.2.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.3.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.4.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.weight": 65536,
  "module.transformer.decoder.layers.5.ca_qpos_sine_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.transformer.pan._downsample.0.weight": 589824,
  "module.transformer.pan._downsample.0.bias": 256,
  "module.transformer.pan._downsample.1.weight": 589824,
  "module.transformer.pan._downsample.1.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[12/09 14:24:16.046]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[12/09 14:24:16.047]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50 --batch_size 2 --epochs 50 --lr_drop 40 --coco_path /home/mmcom/COCO --use_dn --find_unused_params
[12/09 14:24:16.047]: world size: 2
[12/09 14:24:16.047]: rank: 1
[12/09 14:24:16.048]: local_rank: 1
[12/09 14:24:16.048]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=50, eval=False, find_unused_params=True, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.001, lr_backbone=1e-05, lr_drop=40, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[12/09 14:24:18.237]: number of params:38142913
[12/09 14:24:18.242]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.transformer.pan._downsample.0.weight": 589824,
  "module.transformer.pan._downsample.0.bias": 256,
  "module.transformer.pan._downsample.1.weight": 589824,
  "module.transformer.pan._downsample.1.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[12/09 14:26:42.767]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[12/09 14:26:42.768]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50 --batch_size 2 --epochs 50 --lr_drop 40 --coco_path /home/mmcom/COCO --use_dn --find_unused_params
[12/09 14:26:42.768]: world size: 2
[12/09 14:26:42.768]: rank: 1
[12/09 14:26:42.768]: local_rank: 1
[12/09 14:26:42.769]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=50, eval=False, find_unused_params=True, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.001, lr_backbone=1e-05, lr_drop=40, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[12/09 14:26:44.987]: number of params:38142913
[12/09 14:26:44.992]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.transformer.pan._downsample.0.weight": 589824,
  "module.transformer.pan._downsample.0.bias": 256,
  "module.transformer.pan._downsample.1.weight": 589824,
  "module.transformer.pan._downsample.1.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[12/09 14:28:44.829]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[12/09 14:28:44.830]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50 --batch_size 2 --epochs 50 --lr_drop 40 --coco_path /home/mmcom/COCO --use_dn --find_unused_params
[12/09 14:28:44.830]: world size: 2
[12/09 14:28:44.830]: rank: 1
[12/09 14:28:44.830]: local_rank: 1
[12/09 14:28:44.831]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=50, eval=False, find_unused_params=True, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.001, lr_backbone=1e-05, lr_drop=40, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[12/09 14:28:47.338]: number of params:39228481
[12/09 14:28:47.347]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 32768,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 128,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.weight": 16384,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.bias": 64,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 32768,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 128,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.weight": 16384,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.bias": 64,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 32768,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 128,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.weight": 16384,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.bias": 64,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 32768,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 128,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.weight": 16384,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.bias": 64,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 32768,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 128,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.weight": 16384,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.bias": 64,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 32768,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 128,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.weight": 16384,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.bias": 64,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.transformer.pan._downsample.0.weight": 589824,
  "module.transformer.pan._downsample.0.bias": 256,
  "module.transformer.pan._downsample.1.weight": 589824,
  "module.transformer.pan._downsample.1.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[12/09 14:31:04.636]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[12/09 14:31:04.637]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50 --batch_size 2 --epochs 50 --lr_drop 40 --coco_path /home/mmcom/COCO --use_dn --find_unused_params
[12/09 14:31:04.637]: world size: 2
[12/09 14:31:04.637]: rank: 1
[12/09 14:31:04.637]: local_rank: 1
[12/09 14:31:04.638]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=50, eval=False, find_unused_params=True, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.001, lr_backbone=1e-05, lr_drop=40, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[12/09 14:31:07.085]: number of params:45533761
[12/09 14:31:07.091]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 32768,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 128,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.weight": 16384,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.bias": 64,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 32768,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 128,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.weight": 16384,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.bias": 64,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 32768,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 128,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.weight": 16384,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.bias": 64,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 32768,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 128,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.weight": 16384,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.bias": 64,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 32768,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 128,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.weight": 16384,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.bias": 64,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 32768,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 128,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.weight": 16384,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.bias": 64,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.transformer.pan._downsample.0.weight": 589824,
  "module.transformer.pan._downsample.0.bias": 256,
  "module.transformer.pan._downsample.1.weight": 589824,
  "module.transformer.pan._downsample.1.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[12/09 14:35:07.727]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[12/09 14:35:07.728]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50 --batch_size 2 --epochs 50 --lr_drop 40 --coco_path /home/mmcom/COCO --use_dn --find_unused_params
[12/09 14:35:07.728]: world size: 2
[12/09 14:35:07.728]: rank: 1
[12/09 14:35:07.728]: local_rank: 1
[12/09 14:35:07.729]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=50, eval=False, find_unused_params=True, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=0.001, lr_backbone=1e-05, lr_drop=40, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[12/09 14:35:10.170]: number of params:45533761
[12/09 14:35:10.176]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 32768,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 128,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.weight": 16384,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.bias": 64,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 32768,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 128,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.weight": 16384,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.bias": 64,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 32768,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 128,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.weight": 16384,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.bias": 64,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 32768,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 128,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.weight": 16384,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.bias": 64,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 32768,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 128,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.weight": 16384,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.bias": 64,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 32768,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 128,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.weight": 16384,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.bias": 64,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.transformer.pan._downsample.0.weight": 589824,
  "module.transformer.pan._downsample.0.bias": 256,
  "module.transformer.pan._downsample.1.weight": 589824,
  "module.transformer.pan._downsample.1.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
[12/15 12:19:11.625]: git:
  sha: c611bd10ffd585c04d8f56e53db1dec948674804, status: has uncommited changes, branch: main

[12/15 12:19:11.638]: Command: main.py --local_rank=1 -m detr_mul --output_dir logs/mul_detr/R50 --batch_size 2 --epochs 50 --lr_drop 40 --coco_path /home/mmcom/COCO --use_dn --find_unused_params
[12/15 12:19:11.638]: world size: 2
[12/15 12:19:11.638]: rank: 1
[12/15 12:19:11.639]: local_rank: 1
[12/15 12:19:11.639]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5, box_noise_scale=0.4, clip_max_norm=0.1, cls_loss_coef=1, coco_panoptic_path=None, coco_path='/home/mmcom/COCO', contrastive=False, dataset_file='coco', debug=False, dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, drop_lr_now=False, dropout=0.1, enc_layers=6, enc_n_points=4, eos_coef=0.1, epochs=50, eval=False, find_unused_params=True, finetune_ignore=None, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, gpu=1, hidden_dim=256, label_noise_scale=0.3, local_rank=1, lr=5e-05, lr_backbone=1e-05, lr_drop=40, mask_loss_coef=1, masks=False, modelname='detr_mul', nheads=8, note='', num_feature_levels=4, num_patterns=0, num_queries=300, num_results=300, num_select=300, num_workers=2, output_dir='logs/mul_detr/R50', override_resumed_lr_drop=False, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, random_refpoints_xy=False, rank=1, remove_difficult=False, resume='', return_interm_layers=False, save_checkpoint_interval=10, save_log=False, save_results=False, scalar=5, seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, start_epoch=0, transformer_activation='prelu', two_stage=False, use_dn=True, use_lft=False, use_mqs=False, weight_decay=0.0001, world_size=2)

[12/15 12:19:15.046]: number of params:45533761
[12/15 12:19:15.052]: params:
{
  "module.transformer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.0.activation.weight": 1,
  "module.transformer.encoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.1.activation.weight": 1,
  "module.transformer.encoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.2.activation.weight": 1,
  "module.transformer.encoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.3.activation.weight": 1,
  "module.transformer.encoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.4.activation.weight": 1,
  "module.transformer.encoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.encoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.encoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.encoder.layers.5.activation.weight": 1,
  "module.transformer.encoder.query_scale.layers.0.weight": 65536,
  "module.transformer.encoder.query_scale.layers.0.bias": 256,
  "module.transformer.encoder.query_scale.layers.1.weight": 65536,
  "module.transformer.encoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.0.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.0.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 32768,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 128,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.weight": 16384,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.bias": 64,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.0.activation.weight": 1,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.1.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.1.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 32768,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 128,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.weight": 16384,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.bias": 64,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.1.activation.weight": 1,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.2.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.2.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 32768,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 128,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.weight": 16384,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.bias": 64,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.2.activation.weight": 1,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.3.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.3.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 32768,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 128,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.weight": 16384,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.bias": 64,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.3.activation.weight": 1,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.4.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.4.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 32768,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 128,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.weight": 16384,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.bias": 64,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.4.activation.weight": 1,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_qpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_qpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kcontent_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_kpos_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_kpos_proj.bias": 256,
  "module.transformer.decoder.layers.5.sa_v_proj.weight": 65536,
  "module.transformer.decoder.layers.5.sa_v_proj.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 32768,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 128,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.weight": 16384,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.bias": 64,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.layers.5.activation.weight": 1,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.query_scale.layers.0.weight": 65536,
  "module.transformer.decoder.query_scale.layers.0.bias": 256,
  "module.transformer.decoder.query_scale.layers.1.weight": 65536,
  "module.transformer.decoder.query_scale.layers.1.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.0.weight": 65536,
  "module.transformer.decoder.ref_anchor_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_anchor_head.layers.1.weight": 512,
  "module.transformer.decoder.ref_anchor_head.layers.1.bias": 2,
  "module.transformer.decoder.bbox_embed.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.layers.2.bias": 4,
  "module.transformer.fpn.lateral_layers.0.weight": 65536,
  "module.transformer.fpn.lateral_layers.0.bias": 256,
  "module.transformer.fpn.lateral_layers.1.weight": 262144,
  "module.transformer.fpn.lateral_layers.1.bias": 256,
  "module.transformer.fpn.lateral_layers.2.weight": 131072,
  "module.transformer.fpn.lateral_layers.2.bias": 256,
  "module.transformer.fpn.output_layers.0.weight": 589824,
  "module.transformer.fpn.output_layers.0.bias": 256,
  "module.transformer.fpn.output_layers.1.weight": 589824,
  "module.transformer.fpn.output_layers.1.bias": 256,
  "module.transformer.fpn.output_layers.2.weight": 589824,
  "module.transformer.fpn.output_layers.2.bias": 256,
  "module.transformer.pan._downsample.0.weight": 589824,
  "module.transformer.pan._downsample.0.bias": 256,
  "module.transformer.pan._downsample.1.weight": 589824,
  "module.transformer.pan._downsample.1.bias": 256,
  "module.class_embed.weight": 23296,
  "module.class_embed.bias": 91,
  "module.label_enc.weight": 23460,
  "module.refpoint_embed.weight": 1200,
  "module.input_proj.weight": 524288,
  "module.input_proj.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
